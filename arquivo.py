# -*- coding: utf-8 -*-
"""MVP_KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PmEglRh__gl69f7Kk5su_65l0e8lQrQh

## Engenharia de Sistemas de Software Inteligentes - Profs. Marcos Kalinowski e Tatiana Escovedo
## Aula 02: Prática de Machine Learning em Python
"""

# configuração para não exibir os warnings
import warnings
warnings.filterwarnings("ignore")

# Imports necessários
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
# Machine Learning
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

"""## Carga do Dataset"""

# Informa a URL de importação do dataset
url = "https://raw.githubusercontent.com/MartinsJoaoPedro/DataSet/main/estudo_covid.csv"

# Lê o arquivo
dataset = pd.read_csv(url, delimiter=',')

# Substitui 'POSITIVO' por 1 e 'NEGATIVO' por 0
dataset = dataset.replace({'POSITIVO': 1, 'NEGATIVO': 0})

# Remove as linhas com valores NaN
dataset = dataset.dropna()

# Mostra as primeiras linhas do dataset
dataset.head(10)

"""## Separação em conjunto de treino e conjunto de teste com holdout"""

test_size = 0.20 # tamanho do conjunto de teste
seed = 7 # semente aleatória

# Separação em conjuntos de treino e teste
array = dataset.values

# Entrada
X = np.hstack((array[:,0:1], array[:,2:7])) # Todas as colunas exceto a segunda
# Saida
y = array[:,1] # Coluna 2
y = y.astype(int)

print("Valores de entrada X:")
print(X)

print("\nValores de saída y:")
print(y)

X_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=test_size, shuffle=True, random_state=seed, stratify=y) # holdout com estratificação

# Parâmetros e partições da validação cruzada
scoring = 'accuracy'
num_particoes = 10
kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed) # validação cruzada com estratificação

"""## Modelagem e Inferência

### Criação e avaliação de modelos: linha base
"""

np.random.seed(7) # definindo uma semente global

# Lista que armazenará os modelos
models = []

# Criando os modelos e adicionando-os na lista de modelos
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))

# Listas para armazenar os resultados
results = []
names = []

# Avaliação dos modelos
for name, model in models:
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

# Boxplot de comparação dos modelos
fig = plt.figure(figsize=(15,10))
fig.suptitle('Comparação dos Modelos')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

"""### Criação e avaliação de modelos: dados padronizados e normalizados"""

np.random.seed(7) # definindo uma semente global para este bloco

# Listas para armazenar os armazenar os pipelines e os resultados para todas as visões do dataset
pipelines = []
results = []
names = []


# Criando os elementos do pipeline

# Algoritmos que serão utilizados
knn = ('KNN', KNeighborsClassifier())
cart = ('CART', DecisionTreeClassifier())
naive_bayes = ('NB', GaussianNB())
svm = ('SVM', SVC())

# Transformações que serão utilizadas
standard_scaler = ('StandardScaler', StandardScaler())
min_max_scaler = ('MinMaxScaler', MinMaxScaler())


# Montando os pipelines

# Dataset original
pipelines.append(('KNN-orig', Pipeline([knn])))
pipelines.append(('CART-orig', Pipeline([cart])))
pipelines.append(('NB-orig', Pipeline([naive_bayes])))
pipelines.append(('SVM-orig', Pipeline([svm])))

# Dataset Padronizado
pipelines.append(('KNN-padr', Pipeline([standard_scaler, knn])))
pipelines.append(('CART-padr', Pipeline([standard_scaler, cart])))
pipelines.append(('NB-padr', Pipeline([standard_scaler, naive_bayes])))
pipelines.append(('SVM-padr', Pipeline([standard_scaler, svm])))

# Dataset Normalizadochrome-extension://mcgbeeipkmelnpldkobichboakdfaeon/images/logo-vertical.svg
pipelines.append(('KNN-norm', Pipeline([min_max_scaler, knn])))
pipelines.append(('CART-norm', Pipeline([min_max_scaler, cart])))
pipelines.append(('NB-norm', Pipeline([min_max_scaler, naive_bayes])))
pipelines.append(('SVM-norm', Pipeline([min_max_scaler, svm])))

# Executando os pipelines
for name, model in pipelines:
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %.3f (%.3f)" % (name, cv_results.mean(), cv_results.std()) # formatando para 3 casas decimais
    print(msg)

# Boxplot de comparação dos modelos
fig = plt.figure(figsize=(25,6))
fig.suptitle('Comparação dos Modelos - Dataset orginal, padronizado e normalizado')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names, rotation=90)
plt.show()

"""### Otimização dos hiperparâmetros"""

# Tuning do KNN

np.random.seed(7) # definindo uma semente global para este bloco

pipelines = []

# Definindo os componentes do pipeline
knn = ('KNN', KNeighborsClassifier())
standard_scaler = ('StandardScaler', StandardScaler())
min_max_scaler = ('MinMaxScaler', MinMaxScaler())

pipelines.append(('knn-orig', Pipeline(steps=[knn])))
pipelines.append(('knn-padr', Pipeline(steps=[standard_scaler, knn])))
pipelines.append(('knn-norm', Pipeline(steps=[min_max_scaler, knn])))

param_grid = {
    'KNN__n_neighbors': [1,3,5,7,9,11,13,15,17,19,21],
    'KNN__metric': ["euclidean", "manhattan", "minkowski"],
}

# Prepara e executa o GridSearchCV
for name, model in pipelines:
    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)
    grid.fit(X_train, y_train)
    # imprime a melhor configuração
    print("Sem tratamento de missings: %s - Melhor: %f usando %s" % (name, grid.best_score_, grid.best_params_))

"""## Finalização do Modelo"""

# Avaliação do modelo com o conjunto de testes

# Preparação do modelo
scaler = StandardScaler().fit(X_train) # ajuste do scaler com o conjunto de treino
rescaledX = scaler.transform(X_train) # aplicação da padronização no conjunto de treino
model = KNeighborsClassifier(metric='manhattan', n_neighbors=15)
model.fit(rescaledX, y_train)

# Estimativa da acurácia no conjunto de teste
rescaledTestX = scaler.transform(X_test) # aplicação da padronização no conjunto de teste
predictions = model.predict(rescaledTestX)
print(accuracy_score(y_test, predictions))

# Preparação do modelo com TODO o dataset
scaler = StandardScaler().fit(X) # ajuste do scaler com TODO o dataset
rescaledX = scaler.transform(X) # aplicação da padronização com TODO o dataset
model.fit(rescaledX, y)

"""## Simulando a aplicação do modelo em dados não vistos"""

# Novos dados - não sabemos a classe!

# idade	rt_pcr	leucocitos	basofilos	creatinina	proteina_c	hemoglobina
data = {'idade':  [40, 55, 59],
        'leucocitos': [5000, 6000, 9060],
        'basofilos': [0, 1, 0],
        'creatinina': [0.88, 0.75, 0.86],
        'proteina_c': [141, 263, 45],
        'hemoglobina': [13.4, 14.1, 12.3],
        }

atributos = ['idade', 'leucocitos', 'basofilos', 'creatinina', 'proteina_c', 'hemoglobina']
entrada = pd.DataFrame(data, columns=atributos)

array_entrada = entrada.values
X_entrada = array_entrada[:,0:6].astype(float)  # Seleciona todas as 7 colunas

# Padronização nos dados de entrada usando o scaler utilizado em X
rescaledEntradaX = scaler.transform(X_entrada)
print(rescaledEntradaX)

# Predição de classes dos dados de entrada
saidas = model.predict(rescaledEntradaX)
print(saidas)

"""**Fazendo o download do arquivo KNN pkl**"""

import pickle
from google.colab import files

# Salvar o modelo usando pickle
with open('covid_knn.pkl', 'wb') as pickle_out:
    pickle.dump(model, pickle_out)

# Baixar o arquivo para o ambiente local
files.download('covid_knn.pkl')

"""**Fazendo o download do arquivo joblib**"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from joblib import dump

# Seus dados de treinamento
data = {'idade': [50, 67, 66],
        'leucocitos': [6880, 19830, 8030],
        'basofilos': [0, 0, 1],
        'creatinina': [0.55, 0.61, 0.80],
        'proteina_c': [258, 149, 300],
        'hemoglobina': [10, 9.1, 11.8]
        }

atributos = ['idade', 'leucocitos', 'basofilos', 'creatinina', 'proteina_c', 'hemoglobina']
entrada = pd.DataFrame(data, columns=atributos)

# Extrair as features
newX_train = entrada.values.astype(float)

# Inicializar o StandardScaler
scaler = StandardScaler()

# Ajustar o scaler aos dados de treinamento e transformar os dados
X_train_scaled = scaler.fit_transform(newX_train)

# Salvar o scaler em um arquivo
dump(scaler, 'scaler.joblib')

# Fazer o download do arquivo
files.download('scaler.joblib')

"""**Regressão Linear**"""

import pandas as pd
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from pickle import dump
from pickle import load

# Cria o modelo
modelo = LogisticRegression(solver='liblinear')

# Treina o modelo
modelo.fit(X_train, y_train)

# Salva o modelo no disco
filename = 'covid_lr.pkl'
dump(modelo, open(filename, 'wb'))

"""**Carrega o modelo**"""

# Algum tempo depois...
# Carrega o modelo do disco
loaded_model = load(open(filename, 'rb'))
result = loaded_model.score(X_test, y_test)
print(result)

# Baixar o arquivo para o ambiente local
files.download('covid_lr.pkl')

"""Notebook para gerar os modelos knn, lr e joblib
O Dataset é carregado e recebe um modelo onde o valor positivo é alterado para 1 e nagativo para 0.

Exemplo:

Valores de entrada X:

```
[[68 15910.0 0.0 0.89 136.0 8.6]
 [68 19830.0 0.0 0.77 51.0 11.2]
 [68 13250.0 0.0 0.66 333.0 7.9]
 ...
 [62 32290.0 0.0 5.0 87.0 8.9]
 [62 12100.0 0.0 1.11 24.0 10.6]
 [49 19900.0 0.0 0.92 166.0 16.4]]
```

Valores de saída y:

```
[1 1 0]
```

1 Para valores positivos e 0 Para negativo
Os dados são analisados e normalizados e avalaidos com 78%
É feita uma simualção obtendo:

```
[Positivo, Positivo, Negativo]
```



É feito o download dos modelos.
"""